{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyc9B1LIZOHrEFlouAoefr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smaali1rim/GenAI-with-Google-learn-guide/blob/main/Prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 1 - Prompting"
      ],
      "metadata": {
        "id": "kyXTkHHai9Nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the SDK¶"
      ],
      "metadata": {
        "id": "uxLleDRXmjba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.3\""
      ],
      "metadata": {
        "id": "fO2zrAQkmiMw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython import get_ipython\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "k2t_cdghmomo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the API key"
      ],
      "metadata": {
        "id": "nNsS6VtUtrb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "JUtJUgKYnH4Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose a model"
      ],
      "metadata": {
        "id": "PRF2zn3Mt436"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "-fpwW6z8rFBD",
        "outputId": "3270c40c-b735-482d-e0ee-ddb7a71416f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-exp-0801\n",
            "models/gemini-1.5-pro-exp-0827\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-exp-0827\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-exp-1206\n",
            "models/gemini-exp-1121\n",
            "models/gemini-exp-1114\n",
            "models/learnlm-1.5-pro-experimental\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/aqa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  if model.name == 'models/gemini-1.5-flash':\n",
        "    print(model)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "wH8Xi5-5rIW0",
        "outputId": "30782581-4291-48d7-e59b-784fe1607170"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore generation parameters"
      ],
      "metadata": {
        "id": "yVl8xfWHqFTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output length"
      ],
      "metadata": {
        "id": "y0MyGD35qWMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "short_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
        "\n",
        "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "YfRZ7Ub4n41M",
        "outputId": "c65d18eb-27d9-4d35-d34d-329e2c8e77f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Enduring Significance of Olives in Modern Society\n",
            "\n",
            "The olive, a seemingly unassuming fruit, holds a position of profound importance in modern society, extending far beyond its culinary applications. Its significance is woven into the fabric of history, culture, economics, and even environmental sustainability. From the Mediterranean basin, where it originated, to global markets, the olive and its products – olive oil, olives for table consumption, and olive pomace – have shaped civilizations and continue to play a vital role in shaping our present and future.\n",
            "\n",
            "The historical significance of the olive tree is undeniable.  Its cultivation dates back millennia, with evidence suggesting its domestication in the eastern Mediterranean around 6000 BC.  Ancient civilizations, including the Greeks and Romans, revered the olive tree as a symbol of peace, prosperity, and wisdom.  The olive branch, a universal emblem of peace, originates from the biblical story of Noah's Ark, solidifying its symbolic weight in various religious and cultural contexts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature"
      ],
      "metadata": {
        "id": "49SbiBBEqiON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "high_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
        "\n",
        "\n",
        "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
        "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
        "retry_policy = {\n",
        "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
        "}\n",
        "\n",
        "for _ in range(5):\n",
        "  response = high_temp_model.generate_content('Pick a random country... (respond in a single word)',\n",
        "                                              request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "AJtrOhRNqlH_",
        "outputId": "969da513-bb00-47e9-f671-cca88bb0482f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bhutan\n",
            " -------------------------\n",
            "Kyrgyzstan\n",
            " -------------------------\n",
            "Bhutan\n",
            " -------------------------\n",
            "Bhutan\n",
            " -------------------------\n",
            "Bhutan\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
        "\n",
        "for _ in range(5):\n",
        "  response = low_temp_model.generate_content('Pick a random country... (respond in a single word)',\n",
        "                                             request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "J0neFLPVsObv",
        "outputId": "39354939-a430-4b5a-819e-5eb76465fe22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bhutan\n",
            " -------------------------\n",
            "Bhutan\n",
            " -------------------------\n",
            "Bhutan\n",
            " -------------------------\n",
            "Bhutan\n",
            " -------------------------\n",
            "Bhutan\n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-K and top-P"
      ],
      "metadata": {
        "id": "doeOShxOqn1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Top-K sampling selects the top K most likely tokens from the model’s predicted\n",
        "distribution. The higher top-K, the more creative and varied the model’s output; the\n",
        "lower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to\n",
        "greedy decoding.\n",
        "\n",
        "- Top-P sampling selects the top tokens whose cumulative probability does not exceed\n",
        "a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the\n",
        "LLM’s vocabulary).\n"
      ],
      "metadata": {
        "id": "9Y1gTVwhtcit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        # These are the default values for gemini-1.5-flash-001.\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "    ))\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "GxnJ30e6uKdS",
        "outputId": "2ea8e078-0f16-42d6-dade-f764ea45dbf5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barnaby was a tabby cat of discerning tastes. He preferred the comfort of his plush cushion to the thrill of chasing mice, and the aroma of fresh tuna to the smell of dew-kissed grass. But Barnaby, unlike most cats, had a secret longing for adventure. It bubbled beneath his purrs and flickered in his emerald eyes, a yearning for a world beyond the familiar confines of his home.\n",
            "\n",
            "One day, the window was left ajar, a lapse in the vigilance of Barnaby's human. The enticing scent of freedom wafted in, carrying with it the whisper of the unknown. Barnaby, unable to resist the call, slipped out into the world, his heart pounding with a mixture of excitement and trepidation.\n",
            "\n",
            "The world was a sensory explosion – sights, sounds, smells, all unfamiliar and exhilarating. Barnaby, a seasoned indoor cat, found himself on a journey of discovery. He dodged playful squirrels in the park, marveled at the kaleidoscope of colors in a flower bed, and even had a close encounter with a grumpy old tomcat who gave him a stern warning about staying out of his territory.\n",
            "\n",
            "His journey led him to a bustling alleyway where he befriended a streetwise ginger cat named Ginger. Ginger, seasoned in the ways of the city, taught Barnaby the art of dumpster diving, the thrill of a midnight chase, and the wisdom of staying clear of the grumpy dog who patrolled the corner.\n",
            "\n",
            "As the sun began to set, casting long shadows across the city, a wave of homesickness washed over Barnaby. He missed the comfort of his cushion, the gentle hand that stroked his fur, and the familiar aroma of his tuna-flavored treats. He realized, with a pang of sadness, that adventure was exciting, but home was where he truly belonged.\n",
            "\n",
            "With a renewed sense of purpose, he retraced his steps, Ginger leading the way with a knowing smirk. He found his way back to his window, squeezed through the open space, and was greeted by a frantic but relieved human. \n",
            "\n",
            "Barnaby's adventure was over, but it left an indelible mark on him. He no longer simply looked out the window at the world; he saw it through the lens of his experiences, his heart filled with a quiet understanding of the beauty and wonder that lay beyond his doorstep. And although he still preferred the comfort of his cushion, Barnaby knew, deep down, that a part of him would always be a wild cat, forever yearning for the next adventure. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        # These are the default values for gemini-1.5-flash-001.\n",
        "        temperature=0.5,\n",
        "        top_k=10,\n",
        "        top_p=0.8,\n",
        "    ))\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "NiWGLzpQulNN",
        "outputId": "c3bebd57-2017-4d4f-baa4-9a272426b50c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bartholomew, a ginger tabby with a penchant for mischief and a disdain for routine, was bored. The sunbeams that usually danced across the living room floor were dull, the bird feeder outside was empty, and his human, Emily, was engrossed in a book, oblivious to his plight. This was unacceptable. Bartholomew, a cat of discerning taste, demanded excitement.\n",
            "\n",
            "He surveyed the room, his green eyes landing on the open window. A gust of wind rustled the curtains, whispering tales of adventure. Bartholomew knew what he had to do. He leapt onto the windowsill, his claws finding purchase on the rough wood. With a graceful leap, he was outside.\n",
            "\n",
            "The world was a symphony of smells and sounds. The scent of damp earth, the chirping of unseen birds, the distant rumble of traffic – it was intoxicating. Bartholomew, a seasoned hunter, stalked through the garden, his tail held high. He chased butterflies, stalked a plump robin (who thankfully took flight), and even engaged in a playful battle with a stray dog, much to the dog's surprise.\n",
            "\n",
            "His adventure led him to a bustling street, where he found himself face-to-face with a grumpy-looking pigeon. Bartholomew, ever the charmer, purred and rubbed against the pigeon's leg, earning a disgruntled coo in return. He continued his journey, weaving through legs and dodging shopping bags, until he found himself in a park.\n",
            "\n",
            "The park was a paradise. Squirrels chattered from the trees, children laughed, and the scent of freshly cut grass filled the air. Bartholomew, feeling a pang of hunger, decided to investigate the picnic basket of a young girl. He was just about to make his move when a loud bark startled him. A golden retriever, its tail wagging furiously, bounded towards him.\n",
            "\n",
            "Bartholomew, not one to back down from a challenge, puffed up his fur and hissed. The retriever, taken aback by his bravado, stopped short. The girl, giggling, picked up the dog and held it away from Bartholomew. She offered him a piece of her sandwich, which he accepted with a grateful purr.\n",
            "\n",
            "As the sun began to set, casting long shadows across the park, Bartholomew knew it was time to go home. He had had his fill of adventure, and a warm bed sounded very appealing. He found his way back to the garden, his heart full of the day's experiences.\n",
            "\n",
            "Emily, worried sick, was waiting for him by the window. She scooped him up in her arms, showering him with kisses. Bartholomew, nestled in her embrace, purred contentedly. He had had his adventure, and he knew, with a certainty that only a cat could possess, that more were waiting just around the corner. He was, after all, Bartholomew, the adventurous tabby. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting"
      ],
      "metadata": {
        "id": "HfwYusf3wTob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot\n",
        "\n",
        "Zero-shot prompts are prompts that describe the request for the model directly."
      ],
      "metadata": {
        "id": "VuDpThRhwZy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=5,\n",
        "    ))\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ifXev9C8wL4W",
        "outputId": "b4612b71-5b56-4674-c967-6f63a4b89b39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: **POSITIVE**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enum mode\n",
        "\n",
        "The models are trained to generate text, and can sometimes produce more text than *we* may wish for. In the preceding example, the model will output the label, sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards.\n",
        "\n",
        "The Gemini API has an [Enum mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) feature that allows us to constrain the output to a fixed set of values."
      ],
      "metadata": {
        "id": "IgFwATMWzHSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "z7SEm8vawYSl",
        "outputId": "60dcc7f4-7118-421e-fc71-ea35e98b0663"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-shot and few-shot\n",
        "\n",
        "Providing an example of the expected response is a \"one-shot\" prompt. When  multiple examples are provided, it is a \"few-shot\" prompt."
      ],
      "metadata": {
        "id": "eomNt1ip01QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ))\n",
        "\n",
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "\n",
        "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "K-ixx_JTzWw7",
        "outputId": "068c1047-5e4b-4cac-e840-383d6da1ec0f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"type\": \"normal\",\n",
            "  \"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JSON mode\n",
        "\n",
        "To provide control over the schema, and to ensure that we only receive JSON (with no other text or markdown), we can use the Gemini API's [JSON mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb). This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
      ],
      "metadata": {
        "id": "cfl9A9WL1_h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PdEdzQd61Wd8",
        "outputId": "3b2e7e68-7ac4-43f4-dbda-0def1a731ebb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thought (CoT)\n",
        "\n",
        "Chain-of-Thought prompting is a technique where we instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples."
      ],
      "metadata": {
        "id": "tTAxG67Z2iMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(prompt, request_options=retry_policy)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "H88zbfMt2KGt",
        "outputId": "89b6abbc-6b54-4afe-ba84-a49c628cc028"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "RXHXJKMw3rRL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "347020f8-2f4c-43cc-cf68-b91d0f80b447"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 911.86ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 659.58ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 784.70ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's how to solve this step-by-step:\n",
            "\n",
            "1. **Partner's age when you were 4:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
            "\n",
            "2. **Age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
            "\n",
            "3. **Partner's current age:**  Since you are now 20, and the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct: Reason and act\n"
      ],
      "metadata": {
        "id": "GD0wwMGZ32Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kpfmppzM3xla"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "react_chat = model.start_chat()\n",
        "\n",
        "# You will perform the Action, so generate up to, but not including, the Observation.\n",
        "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
        "\n",
        "resp = react_chat.send_message(\n",
        "    [model_instructions, example1, example2, question],\n",
        "    generation_config=config,\n",
        "    request_options=retry_policy)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "DmVdzYxg6Nbw",
        "outputId": "a499aa2d-2fbf-4367-f753-69c7743856be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the Transformers NLP paper and then find the authors and their ages to determine the youngest one.  I can't directly search for \"youngest author\" as the ages aren't readily available.\n",
            "\n",
            "Action 1\n",
            "<search>Transformers (deep learning model)</search>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}